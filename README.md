# ğŸ§± Kit Fundador v2.0

Un starter kit **agnÃ³stico de lenguaje**, diseÃ±ado para iniciar proyectos de software con **Clean Architecture, DDD, TDD y mejores prÃ¡cticas**, tanto para equipos humanos como para agentes IA.

Este repositorio proporciona el **esqueleto profesional mÃ­nimo** para arrancar un proyecto sin deuda tÃ©cnica inicial, con una estructura clara, tests desde el dÃ­a 1 y un setup robusto que guÃ­a a desarrolladores y agentes hacia decisiones correctas.

---

## âœ¨ Â¿QuÃ© incluye este kit?

âœ”ï¸ Clean Architecture â†’ separaciÃ³n clara Domain / Application / Infrastructure
âœ”ï¸ TDD estricto: estructura de tests lista (unit, integration, e2e)
âœ”ï¸ Scripts de setup robustos (validaciones + modo interactivo)
âœ”ï¸ Observabilidad opcional (Prometheus, Grafana, Jaeger)
âœ”ï¸ Plantillas TypeScript, Python y JSON/Config
âœ”ï¸ DocumentaciÃ³n completa en `dev-docs/`
âœ”ï¸ Perfiles de ejecuciÃ³n EJECUTOR / VALIDADOR para agentes IA
âœ”ï¸ Sistema de guardrails con investigaciÃ³n acadÃ©mica (Chen et al 2024, Liu et al 2024)

> Este README es intencionalmente breve. La documentaciÃ³n extendida estÃ¡ en **[dev-docs/README_FULL.md](dev-docs/README_FULL.md)**.

---

## ğŸš€ Quick Start

```bash
git clone https://github.com/fegome90-cmd/kit_fundador.git mi-proyecto
cd mi-proyecto

chmod +x scripts/setup.sh
./scripts/setup.sh     # o ./scripts/setup.sh --force en CI
```

ElegirÃ¡s entre:

```
1) TypeScript + Node.js (Express + Jest + Prisma)
2) Python (FastAPI + Pytest + SQLAlchemy)
3) JSON/Config only
```

---

## ğŸ“ Estructura mÃ­nima del proyecto

```
proyecto/
â”œâ”€â”€ src/                 # CÃ³digo fuente despuÃ©s del setup
â”‚   â”œâ”€â”€ domain/          # Domain puro (DDD)
â”‚   â”œâ”€â”€ application/     # Use cases
â”‚   â””â”€â”€ infrastructure/  # Frameworks, DB, APIs
â”œâ”€â”€ tests/               # Unit / integration / e2e
â”œâ”€â”€ templates/           # Plantillas de lenguajes
â”œâ”€â”€ dev-docs/            # DocumentaciÃ³n extendida
â”œâ”€â”€ config/              # Reglas, tech stack, observability
â”œâ”€â”€ scripts/             # Automation (setup, validate)
â””â”€â”€ .context/            # Contexto persistente para IA
```

---

## ğŸ“š DocumentaciÃ³n completa

Toda la documentaciÃ³n extendida estÃ¡ organizada en `dev-docs/`:

* **[README_FULL.md](dev-docs/README_FULL.md)** â†’ GuÃ­a completa del proyecto
* **agent-profiles/** â†’ Roles EJECUTOR / VALIDADOR con checklists investigados
* **domain/** â†’ Ubiquitous Language, invariantes, property-based testing
* **plan.md / task.md** â†’ Roadmap y backlog activo
* **context.md** â†’ Contexto y decisiones arquitectÃ³nicas (ADRs)

**Informes de AuditorÃ­a**:
* [AUDITORIA_SETUP_SH.md](AUDITORIA_SETUP_SH.md) â†’ AnÃ¡lisis profundo del script de setup
* [AUDITORIA_TDD_DDD.md](AUDITORIA_TDD_DDD.md) â†’ EvaluaciÃ³n exhaustiva de capacidades TDD/DDD
* [SECURITY_AUDIT_REPORT.md](SECURITY_AUDIT_REPORT.md) â†’ AuditorÃ­a de seguridad completa

---

## ğŸ›¡ï¸ RemediaciÃ³n del setup interactivo

El informe [`AUDITORIA_SETUP_SH.md`](document/informes_CC/AUDITORIA_SETUP_SH.md) detectÃ³ un bloqueo crÃ­tico en la opciÃ³n Python,
19 vulnerabilidades moderadas en la plantilla TypeScript y varios riesgos de usabilidad. Antes de reutilizar `scripts/setup.sh`
en un proyecto real, sigue el [plan de ejecuciÃ³n](dev-docs/setup/setup-sh-remediation-plan.md) que prioriza:

1. **Correcciones crÃ­ticas** (dependencias Python/TypeScript y manejo de errores de `pip`).
2. **Mejoras de usabilidad** (confirmaciÃ³n de sobrescritura, validaciÃ³n de prerequisitos y limpieza de templates).
3. **Hardening opcional** (tests del script, flags verbosos y guardas para `docker-compose`).

> **Estado actual**: âœ… **Fases A y B completadas**. De la **Fase C** ya estÃ¡n operativos el harness Bash (`tests/setup/setup_script.test.sh`, accesible vÃ­a `npm run test:setup`/`make test:setup`) y la integridad de metadatos (`utc_timestamp` + advertencia cuando falta `docker-compose.dev.yml`). La mejora de observabilidad (`--verbose`/`--no-color`) quedÃ³ documentada como **opcional** para que cada consumidor decida si la incorpora.

> ğŸ“¦ DespuÃ©s de cada ejecuciÃ³n, decide quÃ© hacer con `templates/` directamente desde el prompt final; si prefieres evitar preguntas en entornos automatizados, invoca el script con `--force`.

> ğŸ§ª En pipelines sin acceso a npm/PyPI puedes ejecutar `SETUP_SH_SKIP_INSTALLS=true ./scripts/setup.sh` para saltar `npm install`/`pip install` (el harness usa esa variable por defecto) y aun asÃ­ validar el resto del flujo.

Documenta quÃ© fases aplicaste en `dev-docs/task.md` antes de continuar con las tareas principales del roadmap.

## ğŸ—„ï¸ Blueprint de base de datos y migraciones

Aunque el starkit no provisiona una base de datos real, TASK-003 exige que cada equipo defina su propia estrategia de
persistencia. Consulta [`dev-docs/infrastructure/database-blueprint.md`](dev-docs/infrastructure/database-blueprint.md)
para seguir una guÃ­a agnÃ³stica que cubre:

- Servicios recomendados en `docker-compose.dev.yml` (ejemplo con Postgres, adaptable a otros motores).
- Archivos esperados (`.env.example`, `db/migrations/`, seeds) y su relaciÃ³n con `package.json`/`Makefile`.
- Minitareas, revisiones y comandos de testing que puedes usar para adaptar el kit sin aÃ±adir dependencias
  obligatorias.

Completa la checklist del blueprint y actualiza `dev-docs/task.md` cuando definas tu stack real para que el resto del
equipo conozca el estado de TASK-003.

## ğŸ§­ Post-clone Checklist

Este repositorio es un **starkit agnÃ³stico**: incluye ejemplos, no una aplicaciÃ³n completa. DespuÃ©s de clonar, sigue estos pasos
para dejarlo operativo en tu contexto:

1. **Entry point real** â†’ crea el archivo de arranque de tu servicio (`src/index.ts`, `main.py`, etc.) y actualiza los scripts
   (`package.json`, `Makefile`, `docker-compose`) para apuntar a Ã©l.
2. **Dependencias implÃ­citas** â†’ importa manualmente mÃ³dulos como `crypto` y reemplaza los helpers ficticios (`hashed_${plainPassword}`,
   event dispatcher en memoria) por servicios reales.
3. **Tooling** â†’ decide tu stack de lint/test (ESLint, Pytest, Go test, etc.) y actualiza `lint-staged`, hooks y pipelines segÃºn
   corresponda. Consulta la [GuÃ­a de Tooling](dev-docs/tooling-guide.md) para reemplazar los placeholders de `package.json` y
   alinear linters/formatters multi-lenguaje.
4. **DocumentaciÃ³n viva** â†’ completa `dev-docs/context.md`, `dev-docs/plan.md` y `dev-docs/task.md` con las decisiones de tu
   producto.

> ğŸ“„ Consulta `dev-docs/consumer-checklist.md` para una lista detallada y marcable de responsabilidades.

## âœ… ValidaciÃ³n post-adaptaciÃ³n

Cuando todos los placeholders hayan sido reemplazados, ejecuta una Ãºltima pasada de calidad:

1. Corre lint, tests y type-check con los comandos reales de tu stack (no dejes los ejemplos sin verificar).
2. Confirma que los servicios ficticios (hasher, dispatcher, entrypoint) fueron sustituidos y documentados en `dev-docs/context.md`.
3. Sincroniza README, `dev-docs/plan.md` y `dev-docs/task.md` para que reflejen los comandos y responsables actuales.

> ğŸ“„ Usa la [GuÃ­a de ValidaciÃ³n Post-AdaptaciÃ³n](dev-docs/post-adaptation-validation.md) para seguir un checklist completo y registrar hallazgos.

## ğŸ§° Personaliza scripts y linters

Los scripts incluidos en `package.json` apuntan a `src/index.ts`, `dist/index.js` y `scripts/seed.ts`, archivos stub que mantienen
los comandos funcionando desde el primer dÃ­a. Cuando definas tu entry point real, personaliza esos archivos o actualiza los scripts
para apuntar a tu implementaciÃ³n definitiva. Sigue las pautas de `dev-docs/tooling-guide.md` para ajustar los comandos `dev`,
`start`, `seed`, `lint`, `format` y `type-check`, asÃ­ como para extender `lint-staged` si trabajas con mÃºltiples lenguajes.

## ğŸ§ª Suites opcionales multi-lenguaje

- `tests/setup/setup_script.test.sh` es el harness oficial del setup interactivo. Corre `npm run test:setup` o `make test:setup` para validar las tres rutas sin tocar tu Ã¡rbol local; el script usa `SETUP_SH_SKIP_INSTALLS=true` para evitar instalaciones reales en entornos CI.
- `tests/integration/test_setup_script.sh` demuestra cÃ³mo validar assets de las plantillas desde Bash. EjecÃºtalo manualmente o  expÃ³n un script (`npm run test:templates`) si quieres integrarlo al pipeline.
- `tests/unit/python/` contiene ejemplos de Pytest para el value object `Email`. Son ilustrativos y no forman parte del comando  `npm test`; habilÃ­talos creando un script propio (`npm run test:py`) o desde tu `Makefile` si tu stack final usa Python. Para  ejecutarlos directamente basta con instalar tus dependencias (`pip install -r requirements.txt` o equivalente) y correr  `pytest tests/unit/python`. Si no vas a mantener una suite en Python, documenta la decisiÃ³n en `dev-docs/context.md` y borra  la carpeta para evitar ruido en tu pipeline.

## ğŸ§± Plantillas de dominio y eventos

- Los value objects (`Email`, `Password`) usan constantes exportadas (regex, dominios bloqueados, longitud mÃ­nima) para que
  puedas sustituir reglas desde un Ãºnico punto sin tocar la lÃ³gica interna.
- El aggregate `User` sÃ³lo modela operaciones bÃ¡sicas y acumula eventos en memoria; la responsabilidad de despacharlos recae en
  tu capa de aplicaciÃ³n a travÃ©s de un `DomainEventDispatcher` propio.
- Sigue el patrÃ³n `save â†’ publish â†’ clear` para evitar publicar eventos que no llegaron a persistirse.
- El bounded context **Identity & Access** ya estÃ¡ descrito en [`dev-docs/domain/ubiquitous-language.md`](dev-docs/domain/ubiquitous-language.md);
  Ãºsalo como blueprint y duplica la plantilla incluida al aÃ±adir nuevos contextos.

> ğŸ“„ Consulta `dev-docs/domain/domain-integration-points.md` para detalles y un checklist de implementaciÃ³n.

## ğŸ› ï¸ Comandos Principales

```bash
# Setup inicial
./scripts/setup.sh

# Desarrollo
make dev              # Iniciar entorno de desarrollo
make test             # Ejecutar todos los tests
make validate         # Validar arquitectura

# Calidad
make lint             # Ejecutar linter
make format           # Formatear cÃ³digo
```

---

## ğŸ™ Agradecimientos

Este kit no serÃ­a posible sin las ideas, principios y contribuciones intelectuales de:

* **Robert C. Martin** â€” Clean Architecture, SOLID
* **Eric Evans** â€” Domain-Driven Design
* **Martin Fowler** â€” Refactoring, Patterns of Enterprise Application Architecture
* **Kent Beck** â€” Test-Driven Development
* **Michael Nygard** â€” Release It! (resilience patterns)
* **Google SRE Team** â€” Observability, SLOs
* **OWASP Foundation** â€” Seguridad de aplicaciones web

**InvestigaciÃ³n AcadÃ©mica Integrada**:
* Chen et al (2024) â€” "Evaluating Large Language Models Trained on Code"
* Liu et al (2024) â€” "Lost in the Middle: How Language Models Use Long Contexts"

---

## ğŸ“œ Licencia

MIT â€” Ãºsalo libremente para cualquier proyecto.
